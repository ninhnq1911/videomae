{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#VIDEOMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "\n",
    "sys.path.append(\"G:\\\\CODE\\\\VIDEOMAE\\\\videomae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pathlib in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from -r ../requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: huggingface_hub in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from -r ../requirements.txt (line 2)) (0.22.1)\n",
      "Requirement already satisfied: ipywidgets in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from -r ../requirements.txt (line 3)) (8.1.2)\n",
      "Requirement already satisfied: wget in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from -r ../requirements.txt (line 4)) (3.2)\n",
      "Requirement already satisfied: pandas in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from -r ../requirements.txt (line 5)) (2.2.1)\n",
      "Requirement already satisfied: torch in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from -r ../requirements.txt (line 6)) (2.2.2)\n",
      "Requirement already satisfied: torchvision in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from -r ../requirements.txt (line 7)) (0.17.2)\n",
      "Requirement already satisfied: torchaudio in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from -r ../requirements.txt (line 8)) (2.2.2)\n",
      "Requirement already satisfied: pytorchvideo in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from -r ../requirements.txt (line 9)) (0.1.5)\n",
      "Requirement already satisfied: transformers in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from -r ../requirements.txt (line 10)) (4.39.3)\n",
      "Requirement already satisfied: evaluate in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from -r ../requirements.txt (line 11)) (0.4.1)\n",
      "Requirement already satisfied: accelerate in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from -r ../requirements.txt (line 12)) (0.28.0)\n",
      "Requirement already satisfied: imageio in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from -r ../requirements.txt (line 13)) (2.34.0)\n",
      "Requirement already satisfied: filelock in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from huggingface_hub->-r ../requirements.txt (line 2)) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from huggingface_hub->-r ../requirements.txt (line 2)) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from huggingface_hub->-r ../requirements.txt (line 2)) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from huggingface_hub->-r ../requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: requests in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from huggingface_hub->-r ../requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from huggingface_hub->-r ../requirements.txt (line 2)) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from huggingface_hub->-r ../requirements.txt (line 2)) (4.10.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from ipywidgets->-r ../requirements.txt (line 3)) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from ipywidgets->-r ../requirements.txt (line 3)) (8.22.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from ipywidgets->-r ../requirements.txt (line 3)) (5.14.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from ipywidgets->-r ../requirements.txt (line 3)) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from ipywidgets->-r ../requirements.txt (line 3)) (3.0.10)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from pandas->-r ../requirements.txt (line 5)) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from pandas->-r ../requirements.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from pandas->-r ../requirements.txt (line 5)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from pandas->-r ../requirements.txt (line 5)) (2024.1)\n",
      "Requirement already satisfied: sympy in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from torch->-r ../requirements.txt (line 6)) (1.12)\n",
      "Requirement already satisfied: networkx in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from torch->-r ../requirements.txt (line 6)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from torch->-r ../requirements.txt (line 6)) (3.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from torchvision->-r ../requirements.txt (line 7)) (10.3.0)\n",
      "Requirement already satisfied: fvcore in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from pytorchvideo->-r ../requirements.txt (line 9)) (0.1.5.post20221221)\n",
      "Requirement already satisfied: av in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from pytorchvideo->-r ../requirements.txt (line 9)) (12.0.0)\n",
      "Requirement already satisfied: parameterized in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from pytorchvideo->-r ../requirements.txt (line 9)) (0.9.0)\n",
      "Requirement already satisfied: iopath in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from pytorchvideo->-r ../requirements.txt (line 9)) (0.1.10)\n",
      "Requirement already satisfied: regex!=2019.12.17 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from transformers->-r ../requirements.txt (line 10)) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from transformers->-r ../requirements.txt (line 10)) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from transformers->-r ../requirements.txt (line 10)) (0.4.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from evaluate->-r ../requirements.txt (line 11)) (2.18.0)\n",
      "Requirement already satisfied: dill in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from evaluate->-r ../requirements.txt (line 11)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from evaluate->-r ../requirements.txt (line 11)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from evaluate->-r ../requirements.txt (line 11)) (0.70.16)\n",
      "Requirement already satisfied: responses<0.19 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from evaluate->-r ../requirements.txt (line 11)) (0.18.0)\n",
      "Requirement already satisfied: psutil in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from accelerate->-r ../requirements.txt (line 12)) (5.9.8)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from datasets>=2.0.0->evaluate->-r ../requirements.txt (line 11)) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from datasets>=2.0.0->evaluate->-r ../requirements.txt (line 11)) (0.6)\n",
      "Requirement already satisfied: aiohttp in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from datasets>=2.0.0->evaluate->-r ../requirements.txt (line 11)) (3.9.3)\n",
      "Requirement already satisfied: decorator in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 3)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 3)) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 3)) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 3)) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 3)) (2.17.2)\n",
      "Requirement already satisfied: stack-data in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 3)) (0.6.3)\n",
      "Requirement already satisfied: colorama in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r ../requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from requests->huggingface_hub->-r ../requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from requests->huggingface_hub->-r ../requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from requests->huggingface_hub->-r ../requirements.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from requests->huggingface_hub->-r ../requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: yacs>=0.1.6 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from fvcore->pytorchvideo->-r ../requirements.txt (line 9)) (0.1.8)\n",
      "Requirement already satisfied: termcolor>=1.1 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from fvcore->pytorchvideo->-r ../requirements.txt (line 9)) (2.4.0)\n",
      "Requirement already satisfied: tabulate in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from fvcore->pytorchvideo->-r ../requirements.txt (line 9)) (0.9.0)\n",
      "Requirement already satisfied: portalocker in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from iopath->pytorchvideo->-r ../requirements.txt (line 9)) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from jinja2->torch->-r ../requirements.txt (line 6)) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from sympy->torch->-r ../requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r ../requirements.txt (line 11)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r ../requirements.txt (line 11)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r ../requirements.txt (line 11)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r ../requirements.txt (line 11)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r ../requirements.txt (line 11)) (1.9.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 3)) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 3)) (0.2.13)\n",
      "Requirement already satisfied: pywin32>=226 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from portalocker->iopath->pytorchvideo->-r ../requirements.txt (line 9)) (306)\n",
      "Requirement already satisfied: executing>=1.2.0 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 3)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in g:\\code\\videomae\\videomae\\.env\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r ../requirements.txt (line 3)) (0.2.2)\n",
      "Obtaining file:///G:/CODE/VIDEOMAE/videomae/script/mltools\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: mltools\n",
      "  Building editable for mltools (pyproject.toml): started\n",
      "  Building editable for mltools (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for mltools: filename=mltools-0.0.1-0.editable-py3-none-any.whl size=1436 sha256=cbc4cc7093d42a0618612bd13bf2318c85da6d7337cd3764046cd56b94a49b3a\n",
      "  Stored in directory: C:\\Users\\ninhn\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-xyp9xu6q\\wheels\\cb\\17\\a1\\d7aec908c413a07fdd321fa1522a43c967d01fd9ed92c75070\n",
      "Successfully built mltools\n",
      "Installing collected packages: mltools\n",
      "  Attempting uninstall: mltools\n",
      "    Found existing installation: mltools 0.0.1\n",
      "    Uninstalling mltools-0.0.1:\n",
      "      Successfully uninstalled mltools-0.0.1\n",
      "Successfully installed mltools-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -r ../requirements.txt\n",
    "!python -m pip install -e ../script/mltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import Trainer\n",
    "from mltools.utl.file_util import get_subfolders\n",
    "from videomae.eval.inference import run_inference\n",
    "from videomae.train.dataset import load_test_set, load_train_set, load_val_set\n",
    "from videomae.train.compute import collate_fn, compute_metrics\n",
    "from videomae.train.model import get_image_processor, get_model\n",
    "from videomae.train.trainer import get_train_args\n",
    "from videomae.train.transform import init_train_transform, init_val_transform\n",
    "from videomae.train.utl import display_gif, print_sample_info\n",
    "from videomae.config import clip_duration, dataset_dir, model_ckpt, model_local, new_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = get_subfolders(dataset_dir)\n",
    "cls_name = get_subfolders(os.path.join(dataset_dir, subset[0]))\n",
    "id2label = {i: cls for i, cls in enumerate(cls_name)}\n",
    "label2id = {cls: i for i, cls in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = get_image_processor(model_ckpt)\n",
    "model = get_model(model_local, label2id, id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = init_train_transform()\n",
    "val_transform = init_val_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_train_set(dataset_dir, clip_duration, train_transform)\n",
    "val_dataset = load_val_set(dataset_dir, clip_duration, val_transform)\n",
    "test_dataset = load_test_set(dataset_dir, clip_duration, val_transform)\n",
    "\n",
    "print(\"Number of video: \", train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key of a sample data dict_keys(['video', 'video_name', 'video_index', 'clip_index', 'aug_index', 'label'])\n",
      "video torch.Size([3, 16, 224, 224])\n",
      "video_name Rec_11-21-21_018.avi\n",
      "video_index 158\n",
      "clip_index 0\n",
      "aug_index 0\n",
      "label 0\n",
      "Video label: be-khoa-xe\n"
     ]
    }
   ],
   "source": [
    "sample_video = next(iter(train_dataset))\n",
    "print(\"Key of a sample data\", sample_video.keys())\n",
    "print_sample_info(sample_video, id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\CODE\\VIDEOMAE\\videomae\\.env\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_args = get_train_args(train_dataset)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    train_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490c462241ed4884a2a293890bdcacc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate(test_dataset)\n",
      "File \u001b[1;32mg:\\CODE\\VIDEOMAE\\videomae\\.env\\Lib\\site-packages\\transformers\\trainer.py:1771\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1769\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[0;32m   1770\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[1;32m-> 1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1772\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1773\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1775\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1778\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[1;32mg:\\CODE\\VIDEOMAE\\videomae\\.env\\Lib\\site-packages\\transformers\\trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2124\u001b[0m ):\n\u001b[0;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mg:\\CODE\\VIDEOMAE\\videomae\\.env\\Lib\\site-packages\\transformers\\trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3043\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3045\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3047\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mg:\\CODE\\VIDEOMAE\\videomae\\.env\\Lib\\site-packages\\accelerate\\accelerator.py:2001\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2001\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\CODE\\VIDEOMAE\\videomae\\.env\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\CODE\\VIDEOMAE\\videomae\\.env\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(new_model_name)\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "trainer.log_metrics(\"test\", test_results)\n",
    "trainer.save_metrics(\"test\", test_results)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()\n",
    "print(f\"trained model {new_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = get_model(new_model_name, label2id, id2label)\n",
    "sample_test_video = next(iter(test_dataset))\n",
    "print_sample_info(sample_test_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = run_inference(trained_model, sample_test_video)\n",
    "display_gif(sample_test_video[\"video\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#THE END "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
